{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set\n",
    "\n",
    "We have a set consisting of $m$ training examples:\n",
    "\n",
    "$\\qquad X = \\begin{pmatrix}\n",
    "  x_1^{(1)} & x_2^{(1)} & ... & x_n^{(1)} \\\\\n",
    "  x_1^{(2)} & x_2^{(2)} & ... & x_n^{(2)} \\\\\n",
    "  ... \\\\\n",
    "  x_1^{(m)} & x_2^{(m)} & ... & x_n^{(m)} \\\\\n",
    "\\end{pmatrix}_{m \\times n}$\n",
    "\n",
    "and training labels (also known as target values):\n",
    "\n",
    "$\\qquad Y = \\begin{pmatrix} y^{(1)} \\\\ y^{(2)} \\\\ ... \\\\ y^{(m)} \\end{pmatrix}_{m \\times 1}$\n",
    "\n",
    "Each training example $x^{(i)}$ in $X$ is comprised of $n$ features:\n",
    "\n",
    "$\\qquad x^{(i)} = \\begin{pmatrix} x_1^{(i)} & x_2^{(i)} & ... & x_n^{(i)} \\end{pmatrix}_{1 \\times n}$\n",
    "\n",
    "and has a corresponding training label $y^{(i)}$ in $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear regression function is:\n",
    "\n",
    "$\\qquad z(x) = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n$\n",
    "\n",
    "where $w$ is a vector of coefficients (weights):\n",
    "\n",
    "$\\qquad w = \\begin{pmatrix} w_1 & w_2 & ... & w_n \\end{pmatrix}_{1 \\times n}$\n",
    "\n",
    "and $b$ is the bias.\n",
    "\n",
    "Linear regression can be expressed in vector form as:\n",
    "\n",
    "$\\qquad z(x) = x \\cdot w^T + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Prediction Model\n",
    "\n",
    "Linear prediction function is:\n",
    "\n",
    "$\\qquad \\hat{y}(x) = z(x) = x \\cdot w^T + b$\n",
    "\n",
    "Performance of a linear prediction model with weights $w$ and bias $b$ for a given example $x$ and label $y$ is measured using squared error loss function:\n",
    "\n",
    "$\\qquad l(x, y) = (\\hat{y}(x) - y)^2$\n",
    "\n",
    "$\\qquad l(x, y) = ((x \\cdot w^T + b) - y)^2$\n",
    "\n",
    "Average loss across the entire training set $X$, $Y$ &mdash; also known as the Mean Squared Error (MSE) &mdash; is given by the cost function:\n",
    "\n",
    "$\\qquad L = \\dfrac{1}{m} \\displaystyle\\sum_{i=1}^{m}{l(x^{(i)}, y^{(i)})}$\n",
    "\n",
    "$\\qquad L = \\dfrac{1}{m} \\displaystyle\\sum_{i=1}^{m}{(\\hat{y}(x^{(i)}) - y^{(i)})^2}$\n",
    "\n",
    "$\\qquad L = \\dfrac{1}{m} \\displaystyle\\sum_{i=1}^{m}{((x^{(i)} \\cdot w^T + b) - y^{(i)})^2}$\n",
    "\n",
    "The cost function can be expressed in matrix form as:\n",
    "\n",
    "$\\qquad L = \\dfrac{1}{m} \\|(X \\cdot w^T + \\textbf{1}_{m \\times 1} \\cdot b) - Y\\|^2$\n",
    "\n",
    "where $\\textbf{1}_{m \\times 1}$ is a \"matrix of ones\" and $\\|\\ \\|$ denotes Euclidean ($L^2$) norm.\n",
    "\n",
    "The goal is to find parameters $w$ and $b$ that minimize the cost function $L$ with respect to $X$ and $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification Model (Perceptron)\n",
    "\n",
    "Linear classification function is:\n",
    "\n",
    "$\\qquad \\hat{y}(x) = a(z)$\n",
    "\n",
    "$\\qquad z(x) = x \\cdot w^T + b$\n",
    "\n",
    "where $a$ is an activation function and $z$ is the linear regression function.\n",
    "\n",
    "### Multiclass Perceptron\n",
    "\n",
    "Given $k$ classes ($k > 2$), multiclass classification function becomes:\n",
    "\n",
    "$\\qquad \\hat{y}(x) = a(Z)$\n",
    "\n",
    "$Z$ is a vector of $k$ linear regression functions &mdash; one for each class &mdash; and is expressed as:\n",
    "\n",
    "$\\qquad Z(x) = \\begin{pmatrix} z_1(x) & z_2(x) & ... & z_k(x) \\end{pmatrix}_{1 \\times k}$\n",
    "\n",
    "$\\qquad z_j(x) = x \\cdot w_j^T + b_j$\n",
    "\n",
    "where $w_j$ and $b_j$ are the weights and bias for a given function $z_j$.\n",
    "\n",
    "Vectors $w_j$ and values $b_j$ can be combined into weights matrix $W$ and biases vector $b$:\n",
    "\n",
    "$\\qquad W = \\begin{pmatrix} w_1^T & w_2^T & ... & w_k^T \\end{pmatrix} = \\begin{pmatrix}\n",
    "  w_{1,1} & w_{1,2} & ... & w_{1,k} \\\\\n",
    "  w_{2,1} & w_{2,2} & ... & w_{2,k} \\\\\n",
    "  ... \\\\\n",
    "  w_{n,1} & w_{n,2} & ... & w_{k,k} \\\\\n",
    "\\end{pmatrix}_{n \\times k}$\n",
    "\n",
    "$\\qquad b = \\begin{pmatrix} b_1 & b_2 & ... & b_k \\end{pmatrix}_{1 \\times k}$\n",
    "\n",
    "Vector $Z$ can then be expressed in matrix form as:\n",
    "\n",
    "$\\qquad Z(x) = x \\cdot W + b$\n",
    "\n",
    "Perceptron can be broken down into so called $input$ layer and an $output$ layer:\n",
    "\n",
    "<img src=\"perceptron.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification Model\n",
    "\n",
    "Training labels $y$ can take on one of two values:\n",
    "\n",
    "$\\qquad y = \\begin{cases}\n",
    "  0 & negative\\ class \\\\\n",
    "  1 & positive\\ class\n",
    "\\end{cases}$\n",
    "\n",
    "Binary classification model uses sigmoid (also known as logistic) activation function:\n",
    "\n",
    "$\\qquad \\hat{y}(x) = \\sigma(z) = \\dfrac{1}{1 + e^{-z}}$\n",
    "\n",
    "$\\qquad z(x) = x \\cdot w^T + b$\n",
    "\n",
    "where the value of $\\hat{y}$ represents probability (or confidence) of the prediction belonging to the $positive\\ class$.\n",
    "\n",
    "Conversely, the value of $(1 - \\hat{y}(x))$ is the probability of the prediction belonging to the $negative\\ class$.\n",
    "\n",
    "Performance of a binary classification model with weights $w$ and bias $b$ for a given example $x$ and label $y$ is measured using binary cross-entropy loss function:\n",
    "\n",
    "$\\qquad l(x, y) = -(y \\times \\ln \\hat{y} + (1 - y) \\times \\ln (1 - \\hat{y}))$\n",
    "\n",
    "Binary cross-entropy function is a special case of the categorical cross-entropy function described below.\n",
    "\n",
    "Cost function for the training set $X$, $Y$ is the average of all losses:\n",
    "\n",
    "$\\qquad L =  \\dfrac{1}{m} \\displaystyle\\sum_{i=1}^{m}{l(x^{(i)}, y^{(i)})}$\n",
    "\n",
    "$\\qquad L = -\\dfrac{1}{m} \\displaystyle\\sum_{i=1}^{m}{(y \\times \\ln \\hat{y} + (1 - y) \\times \\ln (1 - \\hat{y}))}$\n",
    "\n",
    "Cost function can be expressed in matrix form as:\n",
    "\n",
    "$\\qquad Z = X \\cdot w^T + \\textbf{1}_{m \\times 1} \\cdot b$\n",
    "\n",
    "$\\qquad \\hat{Y} = \\sigma(Z) = \\dfrac{1}{1 + e^{\\circ(-Z)}}$\n",
    "\n",
    "$\\qquad L = -\\dfrac{1}{m} \\times |\\ Y \\circ \\ln \\hat{Y} + (1 - Y) \\circ \\ln (1 - \\hat{Y})\\ |$\n",
    "\n",
    "where $\\textbf{1}_{m \\times 1}$ is a \"matrix of ones\", $\\circ$ indicates Hadamard (element-wise) exponential and multiplication, and $|\\ \\ |$ denotes $L^1$ norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification Model\n",
    "\n",
    "Given $k$ classes, training labels $y$ can take on any of the values:\n",
    "\n",
    "$\\qquad y \\in \\{ 1, 2, ..., k \\}$\n",
    "\n",
    "Training labels are transformed into vectors of size $k$ using \"one-hot encoding\" technique:\n",
    "\n",
    "$\\qquad \\dot{y} = \\begin{pmatrix} [y = 1] & [y = 2] & ... & [y = k] \\end{pmatrix}_{1 \\times k}$\n",
    "\n",
    "where $[\\ ]$ denotes Iverson bracket.\n",
    "\n",
    "Matrix $\\dot{Y}$ is defined as a set all \"one-hot encoded\" labels:\n",
    "\n",
    "$\\qquad \\dot{Y} = \\begin{pmatrix} \\dot{y}^{(1)} \\\\ \\dot{y}^{(2)} \\\\ ... \\\\ \\dot{y}^{(m)} \\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "  y_1^{(1)} & y_2^{(1)} & ... & y_k^{(1)} \\\\\n",
    "  y_1^{(2)} & y_2^{(2)} & ... & y_k^{(2)} \\\\\n",
    "  ... \\\\\n",
    "  y_1^{(m)} & y_2^{(m)} & ... & y_k^{(m)} \\\\\n",
    "\\end{pmatrix}_{m \\times k}$\n",
    "\n",
    "Multiclass classification model uses $softmax$ activation function:\n",
    "\n",
    "$\\qquad \\hat{y}(x) = softmax(Z) = \\begin{bmatrix} p_1 & p_2 & ... & p_k \\end{bmatrix}$\n",
    "\n",
    "$\\qquad Z(x) = x \\cdot W + b$\n",
    "\n",
    "$\\qquad p_i = \\dfrac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}}$\n",
    "\n",
    "where $p_i$ represents probability of the prediction belonging to class $i$.\n",
    "\n",
    "Function $softmax$ has a property where: $\\sum_{i=1}^k{p_i} = 1$\n",
    "\n",
    "Performance of a multiclass classification model with weights $W$ and biases $b$ for a given example $x$ and \"one-hot encoded\" label $\\dot{y}$ is measured using categorical cross-entropy loss function:\n",
    "\n",
    "$\\qquad l(x, y) = -\\displaystyle\\sum_{j=1}^k{(\\dot{y}_j \\times \\ln \\hat{y}_j)}$\n",
    "\n",
    "Cost function for the training set $X$, $Y$ is the average of all losses:\n",
    "\n",
    "$\\qquad L =  \\dfrac{1}{m} \\displaystyle\\sum_{i=1}^{m}{l(x^{(i)}, y^{(i)})}$\n",
    "\n",
    "$\\qquad L = -\\dfrac{1}{m} \\displaystyle\\sum_{i=1}^{m}{\\displaystyle\\sum_{j=1}^k{(\\dot{y}_j^{(i)} \\times \\ln \\hat{y}_j^{(i)})}}$\n",
    "\n",
    "Cost function can be expressed in matrix form as:\n",
    "\n",
    "$\\qquad Z = X \\cdot W + \\textbf{1}_{m \\times 1} \\cdot b$\n",
    "\n",
    "$\\qquad \\hat{Y} = softmax(Z) = e^{\\circ Z} \\oslash (e^{\\circ Z} \\cdot \\textbf{1}_{k \\times k})$\n",
    "\n",
    "$\\qquad L = - \\dfrac{1}{m} \\times |\\ \\dot{Y} \\circ \\ln \\hat{Y} \\ |$\n",
    "\n",
    "where $\\textbf{1}_{m \\times 1}$ and $\\textbf{1}_{k \\times k}$ are \"matrices of ones\", $\\circ$ indicates Hadamard (element-wise) exponential and multiplication, $\\oslash$ is Hadamard (element-wise) division and $|\\ \\ |$ denotes $L^1$ norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Sigmoid (Logistic) activation function:\n",
    "\n",
    "$\\qquad \\sigma(x) = \\dfrac{1}{1 + e^{-x}}$\n",
    "\n",
    "Hyperbolic tangent activation function:\n",
    "\n",
    "$\\qquad tanh(x) = \\dfrac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "\n",
    "ReLU activation function:\n",
    "\n",
    "$\\qquad relu(x) = \\begin{cases}\n",
    "  0 & for\\ x < 0 \\\\\n",
    "  x & for\\ x \\geqslant 0\n",
    "\\end{cases}$\n",
    "\n",
    "$\\qquad relu(x) = max(0, x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP)\n",
    "\n",
    "Multilayer Perceptron contains one or more $hidden$ layers between the $input$ layer and the $output$ layer &mdash; each with a set of linear functions and activation functions:\n",
    "\n",
    "<img src=\"mlp.png\" width=\"35%\">\n",
    "\n",
    "NB: MLP may also have a multiclass output layer."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}