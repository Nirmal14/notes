{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set\n",
    "\n",
    "We have a set consisting of $m$ training examples:\n",
    "\n",
    "$\\qquad X = \\begin{pmatrix}\n",
    "  x_1^{(1)} & x_2^{(1)} & ... & x_n^{(1)} \\\\\n",
    "  x_1^{(2)} & x_2^{(2)} & ... & x_n^{(2)} \\\\\n",
    "  ... \\\\\n",
    "  x_1^{(m)} & x_2^{(m)} & ... & x_n^{(m)} \\\\\n",
    "\\end{pmatrix}_{m \\times n}$\n",
    "\n",
    "and training labels (also known as target values):\n",
    "\n",
    "$\\qquad \\vec{y} = \\begin{pmatrix} y^{(1)} \\\\ y^{(2)} \\\\ ... \\\\ y^{(m)} \\end{pmatrix}_{m \\times 1}$\n",
    "\n",
    "Each training example $\\vec{x}^{(i)}$ in $X$ is comprised of $n$ features:\n",
    "\n",
    "$\\qquad \\vec{x}^{(i)} = \\begin{pmatrix} x_1^{(i)} & x_2^{(i)} & ... & x_n^{(i)} \\end{pmatrix}_{1 \\times n}$\n",
    "\n",
    "and has a corresponding training label $y^{(i)}$ in $\\vec{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear regression function is:\n",
    "\n",
    "$\\qquad z(\\vec{x}) = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b$\n",
    "\n",
    "where $\\vec{w}$ is a vector of coefficients (weights):\n",
    "\n",
    "$\\qquad \\vec{w} = \\begin{pmatrix} w_1 & w_2 & ... & w_n \\end{pmatrix}_{1 \\times n}$\n",
    "\n",
    "and $b$ is the bias.\n",
    "\n",
    "Linear regression can be expressed in vector form as:\n",
    "\n",
    "$\\qquad z(\\vec{x}) = \\vec{x} \\cdot \\vec{w}^T + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Prediction Model\n",
    "\n",
    "Linear prediction function is:\n",
    "\n",
    "$\\qquad \\hat{y}(\\vec{x}) = z(\\vec{x}) = \\vec{x} \\cdot \\vec{w}^T + b$\n",
    "\n",
    "Performance of a linear prediction model with weights $\\vec{w}$ and bias $b$ for a given example $\\vec{x}$ and label $y$ is measured using squared error loss function:\n",
    "\n",
    "$\\qquad l(\\vec{x}, y) = (\\hat{y}(\\vec{x}) - y)^2$\n",
    "\n",
    "$\\qquad l(\\vec{x}, y) = ((\\vec{x} \\cdot \\vec{w}^T + b) - y)^2$\n",
    "\n",
    "Average loss across the entire training set $X$, $\\vec{y}$ &mdash; also known as the Mean Squared Error (MSE) &mdash; is given by the cost function:\n",
    "\n",
    "$\\qquad L = \\dfrac{1}{m} \\displaystyle\\sum_{i=1}^{m}{l(\\vec{x}^{(i)}, y^{(i)})}$\n",
    "\n",
    "$\\qquad L = \\dfrac{1}{m} \\displaystyle\\sum_{i=1}^{m}{(\\hat{y}(\\vec{x}^{(i)}) - y^{(i)})^2}$\n",
    "\n",
    "$\\qquad L = \\dfrac{1}{m} \\displaystyle\\sum_{i=1}^{m}{((\\vec{x}^{(i)} \\cdot \\vec{w}^T + b) - y^{(i)})^2}$\n",
    "\n",
    "The cost function can be expressed in matrix form as:\n",
    "\n",
    "$\\qquad L = \\dfrac{1}{m} \\|(X \\cdot \\vec{w}^T + \\textbf{1}_{m \\times 1} \\cdot b) - \\vec{y}\\|^2$\n",
    "\n",
    "where $\\textbf{1}_{m \\times 1}$ is a \"matrix of ones\" and $\\|\\ \\|$ denotes Euclidean ($L^2$) norm.\n",
    "\n",
    "The goal is to find parameters $\\vec{w}$ and $b$ that minimize the cost function $L$ with respect to $X$ and $\\vec{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python equivalent\n",
    "y_hat = X @ w.T + b\n",
    "L = np.sum((y_hat - y) ** 2) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification Model (Perceptron)\n",
    "\n",
    "Linear classification function is:\n",
    "\n",
    "$\\qquad \\hat{y}(\\vec{x}) = a(z)$\n",
    "\n",
    "$\\qquad z(\\vec{x}) = \\vec{x} \\cdot \\vec{w}^T + b$\n",
    "\n",
    "where $a$ is an activation function and $z$ is the linear regression function.\n",
    "\n",
    "### Multiclass Perceptron\n",
    "\n",
    "Given $k$ classes ($k > 2$), multiclass classification function becomes:\n",
    "\n",
    "$\\qquad \\vec{\\hat{y}}(\\vec{x}) = a(\\vec{z})$\n",
    "\n",
    "$\\vec{z}$ is a vector of $k$ linear regression functions &mdash; one for each class &mdash; and is expressed as:\n",
    "\n",
    "$\\qquad \\vec{z}(\\vec{x}) = \\begin{pmatrix} z_1(\\vec{x}) & z_2(\\vec{x}) & ... & z_k(\\vec{x}) \\end{pmatrix}_{1 \\times k}$\n",
    "\n",
    "$\\qquad z_j(\\vec{x}) = \\vec{x} \\cdot \\vec{w}_j^T + b_j$\n",
    "\n",
    "where $\\vec{w}_j$ and $b_j$ are the weights and bias for a given function $z_j$.\n",
    "\n",
    "Vectors $\\vec{w}_j$ and values $b_j$ can be combined into weights matrix $W$ and biases vector $\\vec{b}$:\n",
    "\n",
    "$\\qquad W = \\begin{pmatrix} \\vec{w}_1^T & \\vec{w}_2^T & ... & \\vec{w}_k^T \\end{pmatrix} = \\begin{pmatrix}\n",
    "  w_{1,1} & w_{1,2} & ... & w_{1,k} \\\\\n",
    "  w_{2,1} & w_{2,2} & ... & w_{2,k} \\\\\n",
    "  ... \\\\\n",
    "  w_{n,1} & w_{n,2} & ... & w_{k,k} \\\\\n",
    "\\end{pmatrix}_{n \\times k}$\n",
    "\n",
    "$\\qquad \\vec{b} = \\begin{pmatrix} b_1 & b_2 & ... & b_k \\end{pmatrix}_{1 \\times k}$\n",
    "\n",
    "Vector $\\vec{z}$ can then be expressed in matrix form as:\n",
    "\n",
    "$\\qquad \\vec{z}(\\vec{x}) = \\vec{x} \\cdot W + \\vec{b}$\n",
    "\n",
    "Perceptron can be broken down into so called $input$ layer and an $output$ layer:\n",
    "\n",
    "![Perceptron](perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification Model\n",
    "\n",
    "Training labels $y$ can take on one of two values:\n",
    "\n",
    "$\\qquad y = \\begin{cases}\n",
    "  0 & negative\\ class \\\\\n",
    "  1 & positive\\ class\n",
    "\\end{cases}$\n",
    "\n",
    "Binary classification model uses sigmoid (also known as logistic) activation function:\n",
    "\n",
    "$\\qquad \\hat{y}(\\vec{x}) = \\sigma(z) = \\dfrac{1}{1 + e^{-z}}$\n",
    "\n",
    "$\\qquad z(\\vec{x}) = \\vec{x} \\cdot \\vec{w}^T + b$\n",
    "\n",
    "where the value of $\\hat{y}$ represents probability (or confidence) of the prediction belonging to the $positive\\ class$.\n",
    "\n",
    "Conversely, the value of $(1 - \\hat{y})$ is the probability of the prediction belonging to the $negative\\ class$.\n",
    "\n",
    "Performance of a binary classification model with weights $\\vec{w}$ and bias $b$ for a given example $\\vec{x}$ and label $y$ is measured using binary cross-entropy loss function:\n",
    "\n",
    "$\\qquad l(\\vec{x}, y) = -(y \\times \\ln \\hat{y} + (1 - y) \\times \\ln (1 - \\hat{y}))$\n",
    "\n",
    "Binary cross-entropy function is a special case of the categorical cross-entropy function described below.\n",
    "\n",
    "Cost function for the training set $X$, $\\vec{y}$ is the average of all losses:\n",
    "\n",
    "$\\qquad L =  \\dfrac{1}{m} \\displaystyle\\sum_{i=1}^{m}{l(\\vec{x}^{(i)}, y^{(i)})}$\n",
    "\n",
    "$\\qquad L = -\\dfrac{1}{m} \\displaystyle\\sum_{i=1}^{m}{(y \\times \\ln \\hat{y} + (1 - y) \\times \\ln (1 - \\hat{y}))}$\n",
    "\n",
    "Cost function can be expressed in matrix form as:\n",
    "\n",
    "$\\qquad \\vec{z} = X \\cdot \\vec{w}^T + \\textbf{1}_{m \\times 1} \\cdot b$\n",
    "\n",
    "$\\qquad \\vec{\\hat{y}} = \\sigma(\\vec{z}) = \\dfrac{1}{1 + e^{\\circ(-\\vec{z})}}$\n",
    "\n",
    "$\\qquad L = -\\dfrac{1}{m} \\times |\\ \\vec{y} \\circ \\ln \\vec{\\hat{y}} + (1 - \\vec{y}) \\circ \\ln (1 - \\vec{\\hat{y}})\\ |$\n",
    "\n",
    "where $\\textbf{1}_{m \\times 1}$ is a \"matrix of ones\", $\\circ$ indicates Hadamard (element-wise) exponential and multiplication, and $|\\ \\ |$ denotes $L^1$ norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python equivalent\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = X @ w.T + b\n",
    "y_hat = sigmoid(z)\n",
    "L = -np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification Model\n",
    "\n",
    "Given $k$ classes, training labels $y$ can take on one of the values:\n",
    "\n",
    "$\\qquad y \\in \\{ 0, 1, ..., k-1 \\}$\n",
    "\n",
    "Training labels are transformed into vectors of size $k$ using \"one-hot encoding\" technique:\n",
    "\n",
    "$\\qquad \\vec{\\dot{y}} = \\begin{pmatrix} [y = 0] & [y = 1] & ... & [y = k-1] \\end{pmatrix}_{1 \\times k}$\n",
    "\n",
    "where $[\\ ]$ denotes Iverson bracket.\n",
    "\n",
    "Matrix $\\dot{Y}$ is defined as a set all \"one-hot encoded\" labels:\n",
    "\n",
    "$\\qquad \\dot{Y} = \\begin{pmatrix} \\vec{\\dot{y}}^{(1)} \\\\ \\vec{\\dot{y}}^{(2)} \\\\ ... \\\\ \\vec{\\dot{y}}^{(m)} \\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "  y_1^{(1)} & y_2^{(1)} & ... & y_k^{(1)} \\\\\n",
    "  y_1^{(2)} & y_2^{(2)} & ... & y_k^{(2)} \\\\\n",
    "  ... \\\\\n",
    "  y_1^{(m)} & y_2^{(m)} & ... & y_k^{(m)} \\\\\n",
    "\\end{pmatrix}_{m \\times k}$\n",
    "\n",
    "Multiclass classification model uses $softmax$ activation function:\n",
    "\n",
    "$\\qquad \\vec{\\hat{y}}(\\vec{x}) = softmax(\\vec{z}) = \\begin{pmatrix} p_1 & p_2 & ... & p_k \\end{pmatrix}$\n",
    "\n",
    "$\\qquad \\vec{z}(\\vec{x}) = \\vec{x} \\cdot W + \\vec{b}$\n",
    "\n",
    "$\\qquad p_i = \\dfrac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}}$\n",
    "\n",
    "where $p_i$ represents probability of the prediction belonging to class $i$.\n",
    "\n",
    "Performance of a multiclass classification model with weights $W$ and biases $\\vec{b}$ for a given example $\\vec{x}$ and \"one-hot encoded\" label $\\vec{\\dot{y}}$ is measured using categorical cross-entropy loss function:\n",
    "\n",
    "$\\qquad l(\\vec{x}, \\vec{\\dot{y}}) = -\\displaystyle\\sum_{j=1}^k{(\\dot{y}_j \\times \\ln \\hat{y}_j)}$\n",
    "\n",
    "Cost function for the training set $X$, $\\dot{Y}$ is the average of all losses:\n",
    "\n",
    "$\\qquad L =  \\dfrac{1}{m} \\displaystyle\\sum_{i=1}^{m}{l(\\vec{x}^{(i)}, \\vec{\\dot{y}}^{(i)})}$\n",
    "\n",
    "$\\qquad L = -\\dfrac{1}{m} \\displaystyle\\sum_{i=1}^{m}{\\displaystyle\\sum_{j=1}^k{(\\dot{y}_j^{(i)} \\times \\ln \\hat{y}_j^{(i)})}}$\n",
    "\n",
    "Cost function can be expressed in matrix form as:\n",
    "\n",
    "$\\qquad Z = X \\cdot W + \\textbf{1}_{m \\times 1} \\cdot \\vec{b}$\n",
    "\n",
    "$\\qquad \\hat{Y} = softmax(Z) = e^{\\circ Z} \\oslash (e^{\\circ Z} \\cdot \\textbf{1}_{k \\times k})$\n",
    "\n",
    "$\\qquad L = - \\dfrac{1}{m} \\times |\\ \\dot{Y} \\circ \\ln \\hat{Y} \\ |$\n",
    "\n",
    "where $\\textbf{1}_{m \\times 1}$ and $\\textbf{1}_{k \\times k}$ are \"matrices of ones\", $\\circ$ indicates Hadamard (element-wise) exponential and multiplication, $\\oslash$ is Hadamard (element-wise) division and $|\\ \\ |$ denotes $L^1$ norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python equivalent\n",
    "def softmax(z):\n",
    "    ez = np.exp(z)\n",
    "    return ez / np.sum(ez)\n",
    "\n",
    "Y_oh = np.eye(k)[y]\n",
    "\n",
    "Z = X @ W + b\n",
    "Y_hat = softmax(Z)\n",
    "\n",
    "L = -np.sum(Y_oh * np.log(Y_hat)) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Sigmoid (Logistic) activation function:\n",
    "\n",
    "$\\qquad \\sigma(x) = \\dfrac{1}{1 + e^{-x}}$\n",
    "\n",
    "Hyperbolic tangent activation function:\n",
    "\n",
    "$\\qquad tanh(x) = \\dfrac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "\n",
    "ReLU activation function:\n",
    "\n",
    "$\\qquad relu(x) = \\begin{cases}\n",
    "  0 & for\\ x < 0 \\\\\n",
    "  x & for\\ x \\geqslant 0\n",
    "\\end{cases}$\n",
    "\n",
    "$\\qquad relu(x) = max(0, x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP)\n",
    "\n",
    "Multilayer Perceptron contains one or more $hidden$ layers between the $input$ layer and the $output$ layer &mdash; each with a set of linear functions and activation functions:\n",
    "\n",
    "TODO\n",
    "\n",
    "![Multilayer Perceptron](multilayer_perceptron.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}